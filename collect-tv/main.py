import urllib.request
import re #æ­£åˆ™
import os
from datetime import datetime
import requests
import time
from txt2m3u import m3u_to_txt
import sys
import cv2
import ffmpeg
import numpy as np
import subprocess
from bs4 import BeautifulSoup
import zhconv
import uuid
import sys

def time_str(fmt=None):
    if fmt is None:
        fmt = '%Y%m%d%H%M%S'
    return datetime.now().strftime(fmt)
       

def process_name_string(input_str):
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

def process_part(part_str):
    # å¤„ç†é€»è¾‘
    if "CCTV" in part_str:
        part_str=part_str.replace("IPV6", "")  #å…ˆå‰”é™¤IPV6å­—æ ·
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        if not filtered_str.strip(): #å¤„ç†ç‰¹æ®Šæƒ…å†µï¼Œå¦‚æœå‘ç°æ²¡æœ‰æ‰¾åˆ°é¢‘é“æ•°å­—è¿”å›åŸåç§°
            filtered_str=part_str.replace("CCTV", "")
        return "CCTV-"+filtered_str 
        
    elif "å«è§†" in part_str or "è¡›è¦–" in part_str:
        # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ¹é…â€œå«è§†â€åé¢çš„å†…å®¹
        pattern = r'å«è§†ã€Œ.*ã€'
        # ä½¿ç”¨subå‡½æ•°æ›¿æ¢åŒ¹é…çš„å†…å®¹ä¸ºç©ºå­—ç¬¦ä¸²
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    
    return part_str



def tra_sim_convert(text): 
    return zhconv.convert(text, 'zh-hant')


#def verify_link2(link):
#    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'}
#    now=time.time()
#    try:
#        startTime = int(round(time.time() * 1000))
#        res=requests.get(link,headers=headers,timeout=5,stream=True)
#        if res.status_code == 200:
#            for k in res.iter_content(chunk_size=500000):
#                endTime = int(round(time.time() * 1000))
#                useTime = int(endTime - startTime)
#                if k and useTime <= 5000:
#                    return verify_link2(link)
#                elif useTime > 5000:
#                    return False
#                else:
#                    continue
#    except Exception:
#        pass
#    return False


#########################################å›¾ç‰‡æ¯”è¾ƒ
#å‡å€¼å“ˆå¸Œç®—æ³•
def aHash(img):
    #ç¼©æ”¾ä¸º8*8
    img=cv2.resize(img,(8,8),interpolation=cv2.INTER_CUBIC)
    #è½¬æ¢ä¸ºç°åº¦å›¾
    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    #sä¸ºåƒç´ å’Œåˆå€¼ä¸º0ï¼Œhash_strä¸ºhashå€¼åˆå€¼ä¸º''
    s=0
    hash_str=''
    #éå†ç´¯åŠ æ±‚åƒç´ å’Œ
    for i in range(8):
        for j in range(8):
            s=s+gray[i,j]
    #æ±‚å¹³å‡ç°åº¦
    avg=s/64
    #ç°åº¦å¤§äºå¹³å‡å€¼ä¸º1ç›¸åä¸º0ç”Ÿæˆå›¾ç‰‡çš„hashå€¼
    for i in range(8):
        for j in range(8):
            if  gray[i,j]>avg:
                hash_str=hash_str+'1'
            else:
                hash_str=hash_str+'0'
    return hash_str

#å·®å€¼æ„ŸçŸ¥ç®—æ³•
def dHash(img):
    #ç¼©æ”¾8*8
    img=cv2.resize(img,(9,8),interpolation=cv2.INTER_CUBIC)
    #è½¬æ¢ç°åº¦å›¾
    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    hash_str=''
    #æ¯è¡Œå‰ä¸€ä¸ªåƒç´ å¤§äºåä¸€ä¸ªåƒç´ ä¸º1ï¼Œç›¸åä¸º0ï¼Œç”Ÿæˆå“ˆå¸Œ
    for i in range(8):
        for j in range(8):
            if   gray[i,j]>gray[i,j+1]:
                hash_str=hash_str+'1'
            else:
                hash_str=hash_str+'0'
    return hash_str
 
#Hashå€¼å¯¹æ¯”
def cmpHash(hash1,hash2):
    n=0
    #hashé•¿åº¦ä¸åŒåˆ™è¿”å›-1ä»£è¡¨ä¼ å‚å‡ºé”™
    if len(hash1)!=len(hash2):
        return -1
    #éå†åˆ¤æ–­
    for i in range(len(hash1)):
        #ä¸ç›¸ç­‰åˆ™nè®¡æ•°+1ï¼Œnæœ€ç»ˆä¸ºç›¸ä¼¼åº¦
        if hash1[i]!=hash2[i]:
            n=n+1
    return n

#########################################

def check_pic_valid(oper, pic_path):
    hash1 = aHash(cv2.resize(cv2.imread(pic_path), (100, 100)))    
    dirs = os.listdir("./invalid_pic/")
    for file in dirs:
        hash2 = aHash(cv2.resize(cv2.imread('./invalid_pic/' + file), (100, 100)))
        n=cmpHash(hash1,hash2)
        #log=pic_path + " VS " + file + " score = " + str(n)
        #print(log)
        #log_write(oper, log)
        if n < 9:
            return False
    return True

def verify_link(channel_type, oper, channel_name, link, useTime=-1, redirect=False):
    useTime = -1
    try:
        jpg_name = str(uuid.uuid4())
        startTime = int(round(time.time() * 1000))
        output_img_path='./' + oper + '_pic/' + jpg_name + '.jpg'
        ffmpeg_command = 'ffmpeg -i "' + link + '" -vf "select=\'eq(n,0)\'" -err_detect ignore_err -vframes 1 -y ' + output_img_path
        print(ffmpeg_command)
        log = channel_type + "," + channel_name + "," + link + "," + output_img_path
        print(log)
        log_write(oper, log)
        process = subprocess.Popen(ffmpeg_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)  
        stdout, stderr = process.communicate(timeout=20)
        if process.returncode == 0:
            endTime = int(round(time.time() * 1000))
            useTime = int(endTime - startTime)
            if not check_pic_valid(oper, output_img_path) and os.path.exists(output_img_path):
                log = "åˆ é™¤: " + output_img_path
                print(log)
                log_write(oper, log)
                os.remove(output_img_path)
                return ["", useTime]
            log = channel_type + "," + channel_name + "," + link + "," + output_img_path + ",PASS"
            print(log)
            log_write(oper, log)
            return [link, useTime]
        else:
            return ["", useTime]
    except Exception as e:
        print(str(e))
        return ["", useTime]
        

def write_file(str,filename, mode):
    with open(filename, mode, encoding='utf-8') as file:
        file.write(str+"\n")

def log_init(oper):
    write_file("",oper + ".log","w")

def read_file(file_name):
    lines = []
    with open(file_name, 'r', encoding='utf-8') as oper_log:
        lines = oper_log.readlines()
        lines = [x.strip() for x in lines if x.strip()! = '']
    return lines

def log_write(oper,string):
    write_file(string, oper + ".log","a")

def get_uniq_type(ctype):
    global chtype
    for key in chtype.keys():
        for l in chtype[key]:
            if l in ctype:
                return key
    return ctype

def check_exclude(ctype):
    global excludetype
    for ex in excludetype:
        if ex in ctype:
            return True
    return False

def check_exists(mydict, link):    
    for values in mydict.values():
        for line in values:
            channel_address=line.split(",")[1]
            if link == channel_address:
                return True
    return False    

type_mapping = {
    "Entertainment" : "ç¶œè—",
    "Generic" : "ç¶œåˆ",
    "Kids" : "å…’ç«¥",
    "Knowledge" : "çŸ¥è­˜",
    "Movies" : "é›»å½±",
    "News" : "æ–°è",
    "Other" : "å…¶å®ƒ",
    "Sports" : "é‹å‹•",
    "Music" : "éŸ³ä¹",
}

def get_channel_type(type_name):
    if type_name in type_mapping.keys():
        return type_mapping[type_name]
    else:
        return type_name


def process_url(mydict, lines, url):     
    channel_name=""
    channel_address=""
    channel_type="æœªçŸ¥"
    # é€è¡Œå¤„ç†å†…å®¹            
    for line in lines:
        line=line.strip()
        if line.startswith('#'):
            continue
        if  "#genre#" in line and "," in line:
            channel_type=line.split(",")[0].strip()
            channel_type=get_channel_type(get_uniq_type(channel_type))
            continue
        if  "#genre#" not in line and "," in line and ":" in line:
            channel_name=remove_brackets(line.split(',')[0].strip())
            channel_address=line.split(',')[1].strip().replace('https://raw.githubusercontent.com/','https://hub.gitmirror.com/https://raw.githubusercontent.com/')
            if any(sub_string in channel_name for sub_string in ["Playboy"]):
                continue
            if len(channel_name) == 0 or check_exclude(channel_type) or check_exists(mydict, channel_address):
                continue
            if channel_type not in mydict.keys():
                mydict[channel_type] = []
            mydict[channel_type].append(process_name_string(channel_name + "," + channel_address))
            continue


def process_tvname_url(mydict, lines, url, skip=False):     
    channel_name=""
    channel_address=""
    channel_type="æœªåˆ†ç±»"
    # é€è¡Œå¤„ç†å†…å®¹            
    for line in lines:
        line=line.strip()
        if line.startswith('#'):
            continue
        if  "#genre#" in line and "," in line:
            continue
        if  "#genre#" not in line and "," in line and ":" in line:
            channel_name=remove_brackets(line.split(',')[0].strip())
            channel_address=line.split(',')[1].strip().replace('https://raw.githubusercontent.com/','https://hub.gitmirror.com/https://raw.githubusercontent.com/')
            if len(channel_name) == 0 or check_exists(mydict, channel_address):
                continue
            if any(sub_string in channel_name for sub_string in ["Playboy"]):
                continue
            if any(sub_string in channel_name.upper() for sub_string in ["CCTV", 'å¤®è§†']):
                channel_type="å¤®è§†é¢‘é“"
            elif any(sub_string in channel_name.upper() for sub_string in ["å«è§†", 'è¡›è¦–', 'brtv']):
                channel_type="å«è§†é¢‘é“"
            elif any(sub_string in channel_name.lower() for sub_string in ["bloomberg", 'news']):
                channel_type="æ–°è"
            elif any(sub_string in channel_name.lower() for sub_string in ['chcå½±è¿·ç”µå½±', "star", 'sony', 'nick', "mytime", 'fox', "astro", "tvb", 'æ˜Ÿå…‰è§†ç•Œ']):
                channel_type="é›»å½±"
            elif any(sub_string in channel_name.lower() for sub_string in ["ä¸­å¤©", "æ¸¯å°ç”µè§†", "ç·¯ä¾†ç²¾é‡‡", 'çº¢ç‰›', 'å‡¤å‡°', 'æ–°åŠ å¡', 'æ—¥æœ¬', 'æ˜Ÿç©ºå«è§†', 'æ˜Ÿç©ºè¡›è¦–', 'æ˜ ç”»']):
                channel_type="ç¶œåˆ"
            elif any(sub_string in channel_name.lower() for sub_string in ['ç¾å›½', 'é›»å½±', 'æ­¦ä¾ ', 'ç”µå½±', 'å½±è§†', 'æ•…äº‹', 'æ¸¯ç‰‡', 'çƒ­æ’­', 'ç”µè§†å‰§', 'ç™¾å®¶è®²å›', 'çŸ¥å¦çŸ¥å¦', 'ç§‘å¹»', 'çº¢æ¥¼æ¢¦', 'ç»å…¸', 'å‰§é›†', 'å›é€†è€…', 'æ˜“ä¸­å¤©å“ä¸‰å›½']):
                channel_type="ç‰¹è‰²"            
            else:
                channel_type="æœªåˆ†ç±»"
                if skip:
                    continue
            channel_type=get_channel_type(channel_type)
            if channel_type not in mydict.keys():
                mydict[channel_type] = []
            mydict[channel_type].append(process_name_string(channel_name + "," + channel_address))        


#current_directory = os.getcwd()  #å‡†å¤‡è¯»å–txt

#è¯»å–æ–‡æœ¬æ–¹æ³•
#def read_txt_to_array(file_name):
#    try:
#        with open(file_name, 'r', encoding='utf-8') as file:
#            lines = file.readlines()
#            lines = [line.strip() for line in lines]
#            return lines
#    except FileNotFoundError:
#        print(f"File '{file_name}' not found.")
#        return []
#    except Exception as e:
#        print(f"An error occurred: {e}")
#        return []


def verify(channel_type, oper, lines):
    sub_channel={}
    print(channel_type + " é¢‘é“æ•°ï¼š" + str(len(lines)) + "\n")
    cur=1
    for line in lines:
        linesa=line.split(',')
        print("æ­£åœ¨æ£€æŸ¥ï¼š" + str(cur) + "/" + str(len(lines)) + "\n")
        cur = cur + 1
        if len(linesa)<2:
            continue
        channel_name=linesa[0].strip()
        channel_address=linesa[1].strip()
        [redirect_url, useTime] = verify_link(channel_type, oper, channel_name, channel_address)
        if len(redirect_url) > 0 and useTime > -1 and useTime < 15000:
            if channel_name not in sub_channel.keys():
                sub_channel[channel_name]=[]
            else:
                pass
            sub_channel[channel_name].append([channel_name, redirect_url, useTime])
    for k in sub_channel.keys():
        sub_channel[k]=sorted(sub_channel[k], key=lambda t: t[2])
    ch_name_list=sorted(sub_channel.keys())
    print(ch_name_list)
    channel_list = []
    for ch in ch_name_list:
        channel_list = channel_list + [ item_list[0] + ',' + item_list[1] + ',' + str(item_list[2]) for item_list in sub_channel[ch]]
    return channel_list

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæå–æ¯è¡Œä¸­é€—å·å‰é¢çš„æ•°å­—éƒ¨åˆ†ä½œä¸ºæ’åºçš„ä¾æ®
def extract_number(s):
    num_str = s.split(',')[0].split('-')[1]  # æå–é€—å·å‰é¢çš„æ•°å­—éƒ¨åˆ†
    numbers = re.findall(r'\d+', num_str)   #å› ä¸ºæœ‰+å’ŒK
    return int(numbers[-1]) if numbers else 999

# å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰æ’åºå‡½æ•°
def custom_sort(s):
    if "CCTV-4K" in s:
        return 1  # å°†åŒ…å« "4K" çš„å­—ç¬¦ä¸²æ’åœ¨åé¢
    elif "CCTV-8K" in s:
        return 2  # å°†åŒ…å« "8K" çš„å­—ç¬¦ä¸²æ’åœ¨åé¢
    else:
        return 0  # å…¶ä»–å­—ç¬¦ä¸²ä¿æŒåŸé¡ºåº

import re
 
def remove_brackets(s):
    return re.sub(r'\[.*?\]|{.*}|\(.*\)|ã€Œ.*ã€|ã€.*ã€‘|', '', s)

def get_redirect_url(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'
    }
    response = requests.get(url,headers=headers,timeout=10,stream=True)
    return response.url

def delete_tree(root):
    for root, dirs, files in os.walk(root, topdown=False):
        for name in files:
            if not name.endswith("me"):
                os.remove(os.path.join(root, name))
        for name in dirs:
            os.rmdir(os.path.join(root, name))

def check_output_image(oper, channels, ch_type):    
    def check_link_exists_in_array(channels, link):
        for cha in channels:
            if "," + link + "," in cha:
                return True
        return False
    output_img_path=oper + ".log"
    print(output_img_path)
    with open(output_img_path, 'r', encoding='utf-8') as oper_log:
        lines = oper_log.readlines()
        for line in lines:
            linesa=line.split(',')
            if len(linesa) < 4:
                continue
            channel_type=linesa[0].strip()
            channel_name=linesa[1].strip()
            channel_address=linesa[2].strip()            
            channel_img=linesa[3].strip()
            if channel_type == ch_type and os.path.exists(channel_img) and not check_link_exists_in_array(channels, channel_address):
                log="å¢è¡¥:" + channel_type + "," + channel_name + "," + channel_address
                print(log)
                log_write(oper, log)
                channels.append(channel_name + "," + channel_address + ",10")
    return channels


#####å¼€å§‹#################################

# å®šä¹‰è¦è®¿é—®çš„å¤šä¸ªURL
urls = read_file("channels.txt")
m3u_urls = read_file("channels.m3u")

chtype={}
chtype['å«è§†é¢‘é“']=['ğŸ‡¨ğŸ‡³ï½œå«è§†é¢‘é“', 'ğŸ‡¨ğŸ‡³ï½œå«è§†è“å…‰é¢‘é“', 'å«è§†é¢‘é“', 'å«è§†', 'â€¢å«è§†ã€ŒIPV6ã€', 'å«è§†å°', 'æœªçŸ¥']
chtype['å¤®è§†é¢‘é“']=['ğŸ‡¨ğŸ‡³ï½œå¤®è§†é¢‘é“', 'å¤®è§†é¢‘é“', 'CCTV', 'æ•°å­—é¢‘é“', 'ç»¼åˆ', 'å¤®è§†', 'æ•°å­—', 'â€¢å¤®è§†ã€ŒIPV6ã€', 'å¤®è§†å°', 'æ•°å­—ç”µè§†', 'å¤®è§†å…¶ä»–']
chtype['æ¸¯æ¾³å°']=['æ¸¯å°', 'ğŸ‡¨ğŸ‡³ï½œæ¸¯æ¾³å°', 'æ¸¯Â·æ¾³Â·å°', 'æ¸¯æ¾³é¢‘é“', 'å°æ¹¾é¢‘é“', 'æ¸¯æ¾³å°', 'å‡¤å‡°', 'é¦™æ¸¯']
chtype['å½±è§†']=['4K', '4Ké¢‘é“', 'ğŸ‡¨ğŸ‡³ï½œè“å…‰é¢‘é“', 'åŸ‹å †å †', 'ç”µè§†å‰§']
chtype['ä½“è‚²é¢‘é“']=['ğŸ‡¨ğŸ‡³ï½œä½“è‚²é¢‘é“', 'ä½“è‚²é¢‘é“']
chtype['æ˜¥æ™š']=['æ˜¥æ™š', 'å†å¹´æ˜¥æ™š', 'å†å±Šæ˜¥æ™š']
chtype['NEWTV']=['NEWTV', 'â€¢NewTVã€ŒIPV6ã€', 'æœªæ¥ç”µè§†']
chtype['å°‘å„¿åŠ¨ç”»']=['ğŸ‡¨ğŸ‡³ï½œå°‘å„¿åŠ¨ç”»', 'å°‘å„¿åŠ¨ç”»']
chtype['ç”µå°']=['ğŸ‡¨ğŸ‡³â€¢ç”µå°', 'ç”µå°']
chtype['åœ°æ–¹']=['å¹¿ä¸œ', 'æ¹–å—', 'ç¦å»º', 'æ±Ÿè‹', 'äº‘å—', 'å››å·', 'å®‰å¾½', 'å±±è¥¿', 'æ²³åŒ—', 'å¤©æ´¥', 'åŒ—äº¬', 'é‡åº†', 'é™•è¥¿', 'å…¶ä»–é¢‘é“']
chtype['ä¸–ç•Œ']=['ä¸–ç•Œ', 'å›½é™…å°', 'å…¨çƒ']
chtype['è®°å½•']=['è®°å½•', 'è®°å½•ç‰‡']
excludetype=['ç©å¶', 'éº»è±†-MSD', 'rostelekom', 'èƒ¡å¿—è‰¯', 'Adult', 'æˆäººç‚¹æ’­', 'æ—¥æœ¬', 'æ¬§ç¾', 'è‚¥ç¾Š', 'æ›´æ–°æ—¶é—´', 'YouTube', 'ç‰¹è‰²é¢‘é“', 'åŸ‹æ¨æ¨', 'è§£è¯´é¢‘é“', 'è™ç‰™æ–—é±¼', 'â€¢æ¸¸æˆèµ›äº‹', 'æ˜¥æ™š', 'å†å¹´æ˜¥æ™š', 'å†å±Šæ˜¥æ™š', 'BESTV']

def refresh(oper, func):
    #    print("ä½¿ç”¨æ–¹æ³•ï¼š")
    #    print("       python3 main.py init           #åˆå§‹åŒ–å¹¶éªŒè¯githubé¢‘é“åˆ—è¡¨")
    #    print("       python3 main.py checkvalid     #æ£€æŸ¥é¢‘é“æºæœ‰æ•ˆé¢‘é“æ•°")
    #    print("       python3 main.py epgpw download #ä¸‹è½½å¹¶éªŒè¯epg.pwé¢‘é“åˆ—è¡¨") 
    #    print("       python3 main.py epgpw skip     #ä¸ä¸‹è½½å¹¶éªŒè¯epg.pwé¢‘é“åˆ—è¡¨") 
    #    sys.exit()
    mydict = {}
    delete_tree("./" + oper + "_pic/")
    log_init(oper)
    
    if oper == "init":
        files=['dog.txt']
        for f in files:
            with open(f, 'r', encoding='utf-8') as file:
               lines = file.readlines()
               process_url(mydict, lines, f)
    
        #å¾ªç¯å¤„ç†æ¯ä¸ªURL
        for url in urls:
            url=url.replace('https://raw.githubusercontent.com/','https://hub.gitmirror.com/https://raw.githubusercontent.com/')
            #url=url.replace('githubusercontent.com','staticdn.net')
            #æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
            #os.system('wget ' + url + " -O my.txt")
            print("downloading... " + url + "\n")
            os.system('curl ' + url + " -o my.txt")
            with open('my.txt', 'r', encoding='utf-8') as file:
               lines = file.readlines()
               process_url(mydict, lines, url)
        
        #å¾ªç¯å¤„ç†æ¯ä¸ªM3U URL
        for url in m3u_urls:
            url=url.replace('https://raw.githubusercontent.com/','https://hub.gitmirror.com/https://raw.githubusercontent.com/')
            #url=url.replace('githubusercontent.com','staticdn.net')
            #æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
            print("downloading... " + url + "\n")
            os.system('curl ' + url + " -o my.m3u")
            m3u_to_txt('my.m3u', 'my.txt')
            with open('my.txt', 'r', encoding='utf-8') as file:
               lines = file.readlines()
               process_url(mydict, lines, url)
    
        
        #files = os.listdir("./history/")
        #for file in files:
        #    #print(file)
        #    if os.path.isfile("./history/" + file):
        #        with open("./history/" + file, 'r', encoding='utf-8') as file:
        #            lines = file.readlines()
        #            process_url(mydict, lines, file)
        
        
        print("start=======================================================")
        # åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬ï¼ˆå»é‡ï¼Œæ’åºåæ‹¼æ¥ï¼‰
        version=time_str() + ",url"
        all_lines =  ["æ›´æ–°æ—¶é—´,#genre#"] +[version] 
        
        for key in mydict.keys():
            log_write(oper, key)
            channels=verify(key, oper, set(mydict[key]))
            channels=check_output_image(oper, channels, key)
            channels=sorted(channels)
            if len(channels) > 0:
                all_lines = all_lines + ['\n'] + [key + ",#genre#"] + channels
        
        # å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
        output_file = '../../mysite/dog.txt'
        output_file2 = 'dog.txt'
        output_file_bak = './history/dog_' + time_str() + '.txt'
        with open(output_file, 'w', encoding='utf-8') as f, open(output_file_bak, 'w', encoding='utf-8') as fb, open(output_file2, 'w', encoding='utf-8') as f2:
            for line in all_lines:
                linea=line.split(',')
                if len(linea) >= 2:
                    channel_name=linea[0].strip()
                    channel_address=linea[1].strip()                
                    if len(linea) >= 3:
                        channel_source=linea[2].strip()
                        f.write(channel_name + "," + channel_address + '\n')
                        fb.write(channel_name + "," + channel_address + '\n')
                        f2.write(channel_name + "," + channel_address + '\n')
                    else:
                        f.write(channel_name + "," + channel_address + '\n')
                        fb.write(channel_name + "," + channel_address + '\n')
                        f2.write(channel_name + "," + channel_address + '\n')
                else:
                    f.write(line.strip() + '\n')
                    fb.write(line.strip() + '\n')
                    
        print(f"åˆå¹¶åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}, {output_file_bak}")
        print("done=======================================================")
    elif oper == "check":
        files=['test.txt']
        for f in files:
            with open(f, 'r', encoding='utf-8') as file:
               lines = file.readlines()
               process_url(mydict, lines, f)
        
        # åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬ï¼ˆå»é‡ï¼Œæ’åºåæ‹¼æ¥ï¼‰
        version=time_str() + ",url"
        all_lines =  ["æ›´æ–°æ—¶é—´,#genre#"] +[version] 
    
        for key in mydict.keys():
            log_write(oper, key)
            channels=verify(key, oper, set(mydict[key]))
            channels=check_output_image(oper, channels, key)
            channels=sorted(channels)
            if len(channels) > 0:
                print(channels)
                all_lines = all_lines + ['\n'] + [key + ",#genre#"] + channels
    
        print(all_lines)
    
        # å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
        output_file = 'test.txt'
    
        with open(output_file, 'w', encoding='utf-8') as f:
            for line in all_lines:
                linea=line.split(',')
                if len(linea) >= 2:
                    channel_name=linea[0].strip()
                    channel_address=linea[1].strip()                
                    if len(linea) >= 3:
                        channel_source=linea[2].strip()
                        f.write(channel_name + "," + channel_address + '\n')
                    else:
                        f.write(channel_name + "," + channel_address + '\n')
                else:
                    f.write(line.strip() + '\n')                
        print(f"åˆå¹¶åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}")
        print("done=======================================================")
    
    elif oper == "checkvalid":    
        #è·å–æ‰€æœ‰çš„æœ‰æ•ˆçš„é“¾æ¥
        all_valid=[]
        files=['dog.txt', 'output.txt']
        for f in files:
            with open(f, 'r', encoding='utf-8') as file:
               lines = file.readlines()
               for line in lines:
                   linea=line.split(',')
                   if len(linea) >= 2:
                       all_valid.append(linea[1].strip())
        print(all_valid)
        #å¾ªç¯å¤„ç†æ¯ä¸ªURL
        for url in urls:
            url=url.replace('https://raw.githubusercontent.com/','https://hub.gitmirror.com/https://raw.githubusercontent.com/')
            #url=url.replace('githubusercontent.com','staticdn.net')
            #æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
            #os.system('wget ' + url + " -O my.txt")
            print("downloading... " + url + "\n")
            os.system('curl ' + url + " -o my.txt")
            with open('my.txt', 'r', encoding='utf-8') as file:
                valid_count=0
                lines = file.readlines()
                for line in lines:
                    linea=line.split(',')
                    if len(linea) >= 2 and linea[1].strip() != '#genre#' and linea[1].strip() in all_valid:
                       #print(line)
                       valid_count = valid_count + 1
                print(url + ": " + str(valid_count))
        
        #å¾ªç¯å¤„ç†æ¯ä¸ªM3U URL
        for url in m3u_urls:
            url=url.replace('https://raw.githubusercontent.com/','https://hub.gitmirror.com/https://raw.githubusercontent.com/')
            #url=url.replace('githubusercontent.com','staticdn.net')
            #æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
            print("downloading... " + url + "\n")
            os.system('curl ' + url + " -o my.m3u")
            m3u_to_txt('my.m3u', 'my.txt')
            with open('my.txt', 'r', encoding='utf-8') as file:
                lines = file.readlines()
                valid_count=0
                for line in lines:
                    linea=line.split(',')
                    if len(linea) >= 2 and linea[1].strip() != '#genre#' and linea[1].strip() in all_valid:
                       #print(line)
                       valid_count = valid_count + 1
                print(url + ": " + str(valid_count))
    elif oper == "test":
        #print(get_redirect_url('https://stream.freetv.fun/tvbs-4.ctv'))
        #print(get_redirect_url('https://stream.freetv.fun/ph-rock-entertainment-1.ctv'))
        #print(get_redirect_url('https://stream.freetv.fun/hgtv-8.ctv'))
        #print(get_redirect_url('https://stream.freetv.fun/cctv-3-14.m3u8'))
        #print(get_redirect_url('https://stream.freetv.fun/viutv-1.ctv'))
        import uuid
        print((uuid.uuid4()))
        print((uuid.uuid4()))
        print((uuid.uuid4()))
    elif oper == "epgpw":
        all_types=['ä¸­åœ‹å¤§é™¸', 'é¢‘é“ä¸ç¬¦åˆä»»ä½•EPG', 'å°ç£', 'é¦™æ¸¯', 'æ¾³é–€', 'ç¾åœ‹', 'æ–°åŠ å¡', 'è‹±åœ‹', 'æ¾³å¤§åˆ©äº', 'åŠ æ‹¿å¤§', 'æ–°è¥¿è˜­']
        file_prefix = os.path.dirname(os.path.abspath(__file__)) + "/epgpw/"
        if func == "download":
            #ä¸‹è½½æ‰€æœ‰é¢‘é“
            all_links={}
            x = requests.get('https://epg.pw/test_channel_page.html?lang=zh-hans')
            #print(x.text)
            soup = BeautifulSoup(x.text, 'html5lib')
            all_trs = soup.find_all('tr')
            print(os.path.dirname(os.path.abspath(__file__)))
            for tr in all_trs:
                all_tds = tr.find_all('td')
                if len(all_tds) == 3:
                    link=all_tds[0].text.strip()
                    type_name=all_tds[2].text.strip()
                    file_name= file_prefix + all_tds[2].text.strip() + ".txt"
                    if not link.endswith("_original_new.txt") and \
                            link.endswith("_new.txt") and \
                            type_name in all_types and \
                            "banned" not in link and \
                            link not in all_links.values():
                        all_links[type_name]=link                
                        print(all_tds[2].text.strip())
                        print("downloading... " + link + "\n")
                        os.system('curl ' + link + " -o " + file_name) 
                        time.sleep(5)
            for key, value in all_links.items():
                print(key + " : " + value + "\n")
    
    
        files=['gat.txt']
        for f in files:
            with open(f, 'r', encoding='utf-8') as file:
               lines = file.readlines()
               process_url(mydict, lines, f)
               
        ##ä¸­å›½å¤§é™†
        #china_main=['ä¸­åœ‹å¤§é™¸']
        #files=[file_prefix + type + ".txt" for type in  china_main]
        #print(files)
        #for f in files:
        #    with open(f, 'r', encoding='utf-8') as file:
        #        lines = file.readlines()
        #        process_tvname_url(mydict, lines, f)
    
        #å¤§é™†æ¸¯æ¾³å°ä¸–ç•Œ
        gat_types=['ä¸­åœ‹å¤§é™¸', 'å°ç£', 'é¦™æ¸¯', 'æ¾³é–€', 'ç¾åœ‹', 'æ–°åŠ å¡', 'è‹±åœ‹', 'æ¾³å¤§åˆ©äº', 'åŠ æ‹¿å¤§', 'æ–°è¥¿è˜­']
        mydict={}
        files=[file_prefix + type + ".txt" for type in  gat_types]
        print(files)
        for f in files:
            with open(f, 'r', encoding='utf-8') as file:
                lines = file.readlines()
                process_url(mydict, lines, f)
        
        #å…¶ä»–
        unclassfied=['é¢‘é“ä¸ç¬¦åˆä»»ä½•EPG']
        files=[file_prefix + type + ".txt" for type in unclassfied]
        print(files)
        for f in files:
            with open(f, 'r', encoding='utf-8') as file:
                lines = file.readlines()
                process_tvname_url(mydict, lines, f, skip=True)
        
        # åˆå¹¶æ‰€æœ‰å¯¹è±¡ä¸­çš„è¡Œæ–‡æœ¬ï¼ˆå»é‡ï¼Œæ’åºåæ‹¼æ¥ï¼‰
        version=time_str() + ",url"
        all_lines =  ["æ›´æ–°æ—¶é—´,#genre#"] +[version] 
    
        for key in mydict.keys():
            log_write(oper, key)
            channels=verify(key, oper, set(mydict[key]))
            channels=check_output_image(oper, channels, key)
            channels=sorted(channels)
            if len(channels) > 0:
                print(channels)
                all_lines = all_lines + ['\n'] + [key + ",#genre#"] + channels
    
        print(all_lines)
    
        # å°†åˆå¹¶åçš„æ–‡æœ¬å†™å…¥æ–‡ä»¶
        output_file = './history/gat_epgpw_' + time_str() + '.txt'
        output_file2 = 'gat.txt'
        output_mysite_file = "../../mysite/gat.txt"
    
        with open(output_file, 'w', encoding='utf-8') as f, open(output_mysite_file, 'w', encoding='utf-8') as fmysite, open(output_file2, 'w', encoding='utf-8') as f2:
            for line in all_lines:
                linea=line.split(',')
                if len(linea) >= 2:
                    channel_name=linea[0].strip()
                    channel_address=linea[1].strip()                
                    if len(linea) >= 3:
                        channel_source=linea[2].strip()
                        f.write(channel_name + "," + channel_address +  "," + channel_source + '\n')
                        fmysite.write(channel_name + "," + channel_address + '\n')
                        f2.write(channel_name + "," + channel_address + '\n')
                    else:
                        f.write(channel_name + "," + channel_address + '\n')
                        fmysite.write(channel_name + "," + channel_address + '\n')
                        f2.write(channel_name + "," + channel_address + '\n')
                else:
                    f.write(line.strip() + '\n')                
        print(f"åˆå¹¶åçš„æ–‡æœ¬å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file} {output_mysite_file}")
    
    